# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzhWqk7YQ6H8w2naO9MkH1l_lyx6b1pK
"""

!python /content/preprocess_urdu_char_level.py --input-tsv "/content/final_main_dataset.tsv" --output-dir "/content/data_char" --text-column "sentence" --group-column "client_id" --train-ratio 0.8 --val-ratio 0.1 --test-ratio 0.1 --min-chars 5 --max-chars 256 --seed 42

import glob, os
glob.glob("/content/final_main_dataset*.tsv")

import pandas as pd
pd.read_csv("/content/final_main_dataset.tsv", sep="\t", encoding="utf-8").columns

!ls -R /content/data_char

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/preprocess_urdu_char_level.py
# # paste the file contents from the block below here

!python /content/preprocess_urdu_char_level.py --input-tsv "/content/final_main_dataset.tsv" --output-dir "/content/data_char" --text-column "sentence" --group-column "client_id" --train-ratio 0.8 --val-ratio 0.1 --test-ratio 0.1 --min-chars 5 --max-chars 256 --seed 42

!ls -R /content/data_char

import glob; glob.glob("/content/final_main_dataset*.tsv")

import sys, pandas as pd
sys.path.append("/content")
import preprocess_urdu_char_level as prep  # imports functions only; won't run main()

# 1) Show original vs normalized for a few changed rows
df_raw = pd.read_csv("/content/final_main_dataset.tsv", sep="\t", encoding="utf-8", engine="python")
text_col = "sentence"
df_raw = df_raw.dropna(subset=[text_col]).head(500).copy()

rows = []
for s in df_raw[text_col].astype(str).tolist():
    n = prep.normalize_urdu(s)
    if s != n:
        rows.append((s, n))
    if len(rows) >= 5:
        break

print("Samples (original -> normalized):")
for i, (o, n) in enumerate(rows, 1):
    print(f"{i}.")
    print("  original  :", o)
    print("  normalized:", n)

# 2) Demonstrate specific mappings (ك→ک, ي/ى→ی, ه/ة→ہ, أ/إ/آ/ٱ→ا, ئ→ی, ؤ→و)
test = "ك ي ى ه ة أ إ آ ٱ ئ ؤ ، ؛ ؟ ۔ ۰۱۲۳٤٥  ١٢٣"
print("\nMapping demo:")
print(" before:", test)
print(" after :", prep.normalize_urdu(test))

import json

# Load vocab
with open("/content/data_char/vocab/char2id.json", "r", encoding="utf-8") as f:
    char2id = json.load(f)

# Load splits
def read_jsonl(p):
    out = []
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            out.append(json.loads(line))
    return out

val = read_jsonl("/content/data_char/splits/val.jsonl")
test = read_jsonl("/content/data_char/splits/test.jsonl")

val_chars = set("".join(x["text"] for x in val))
test_chars = set("".join(x["text"] for x in test))
train_vocab_chars = set(k for k in char2id.keys() if not k.startswith("<"))  # exclude specials

oov_val = sorted(list(val_chars - train_vocab_chars))
oov_test = sorted(list(test_chars - train_vocab_chars))

print("OOV (val):", oov_val, "count:", len(oov_val))
print("OOV (test):", oov_test, "count:", len(oov_test))

import json

# Load vocab
with open("/content/data_char/vocab/char2id.json", "r", encoding="utf-8") as f:
    char2id = json.load(f)
with open("/content/data_char/vocab/id2char.json", "r", encoding="utf-8") as f:
    id2char = json.load(f)
with open("/content/data_char/vocab/tokenizer_config.json", "r", encoding="utf-8") as f:
    tok_conf = json.load(f)

pad_id = tok_conf["special_token_ids"]["pad_id"]
sos_id = tok_conf["special_token_ids"]["sos_id"]
eos_id = tok_conf["special_token_ids"]["eos_id"]
unk_id = tok_conf["special_token_ids"]["unk_id"]
vocab_size = tok_conf["vocab_size"]

def read_jsonl(p):
    out = []
    with open(p, "r", encoding="utf-8") as f:
        for line in f:
            out.append(json.loads(line))
    return out

train_tok = read_jsonl("/content/data_char/splits/train_tok.jsonl")
val_tok = read_jsonl("/content/data_char/splits/val_tok.jsonl")

# Peek at one example
ex = train_tok[0]
print("length:", ex["length"])
print("ids:", ex["ids"][:80])
print("ids_sos_eos:", ex["ids_sos_eos"][:80])
print("decoded (no sos/eos):", "".join(id2char[i] for i in ex["ids"]))

# Example: build a simple batch with left-padding (or right-padding) as you prefer
def pad_batch(batch_ids, pad_id):
    max_len = max(len(x) for x in batch_ids)
    padded = [x + [pad_id]*(max_len - len(x)) for x in batch_ids]
    attn_mask = [[1]*len(x) + [0]*(max_len - len(x)) for x in batch_ids]  # 1=keep, 0=pad
    return padded, attn_mask

# Build a tiny batch
batch = train_tok[:4]
enc_inputs = [b["ids"] for b in batch]
dec_targets = [b["ids_sos_eos"] for b in batch]  # typical decoder target with sos/eos
enc_padded, enc_mask = pad_batch(enc_inputs, pad_id)
dec_padded, dec_mask = pad_batch(dec_targets, pad_id)

print("enc_padded shape:", (len(enc_padded), len(enc_padded[0])))
print("dec_padded shape:", (len(dec_padded), len(dec_padded[0])))

import math
from typing import Optional, Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F


class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)  # (L, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term[: (d_model // 2)])
        self.register_buffer("pe", pe.unsqueeze(0))  # (1, L, d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, L, d_model)
        x = x + self.pe[:, : x.size(1), :]
        return self.dropout(x)


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.o_proj = nn.Linear(d_model, d_model, bias=False)
        self.attn_drop = nn.Dropout(dropout)
        self.proj_drop = nn.Dropout(dropout)

    def forward(
        self,
        q: torch.Tensor,  # (B, Lq, d_model)
        k: torch.Tensor,  # (B, Lk, d_model)
        v: torch.Tensor,  # (B, Lv, d_model)
        attn_mask: Optional[torch.Tensor] = None,  # broadcastable to (B, nH, Lq, Lk)
    ) -> torch.Tensor:
        B, Lq, _ = q.shape
        Lk = k.size(1)

        q = self.q_proj(q).view(B, Lq, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lq, dH)
        k = self.k_proj(k).view(B, Lk, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lk, dH)
        v = self.v_proj(v).view(B, Lk, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lk, dH)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, nH, Lq, Lk)
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask == 0, float("-inf"))
        attn = torch.softmax(scores, dim=-1)
        attn = self.attn_drop(attn)
        out = torch.matmul(attn, v)  # (B, nH, Lq, dH)
        out = out.transpose(1, 2).contiguous().view(B, Lq, self.d_model)
        out = self.o_proj(out)
        return self.proj_drop(out)


class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.fc2(self.drop(F.gelu(self.fc1(x))))


class EncoderLayer(nn.Module):
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:
        # x: (B, Ls, d_model); src_key_padding_mask: (B, 1, 1, Ls)
        attn_out = self.self_attn(x, x, x, attn_mask=src_key_padding_mask)
        x = self.norm1(x + self.drop(attn_out))
        ff_out = self.ff(x)
        x = self.norm2(x + self.drop(ff_out))
        return x


class DecoderLayer(nn.Module):
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(
        self,
        x: torch.Tensor,  # (B, Lt, d_model)
        mem: torch.Tensor,  # (B, Ls, d_model)
        tgt_mask: Optional[torch.Tensor],  # (B, 1, Lt, Lt)
        src_key_padding_mask: Optional[torch.Tensor],  # (B, 1, 1, Ls)
    ) -> torch.Tensor:
        # masked self-attention
        self_attn_out = self.self_attn(x, x, x, attn_mask=tgt_mask)
        x = self.norm1(x + self.drop(self_attn_out))
        # cross-attention: query=decoder, key/value=encoder memory
        cross_attn_out = self.cross_attn(x, mem, mem, attn_mask=src_key_padding_mask)
        x = self.norm2(x + self.drop(cross_attn_out))
        ff_out = self.ff(x)
        x = self.norm3(x + self.drop(ff_out))
        return x


class TransformerSeq2Seq(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 256,
        n_heads: int = 2,
        num_encoder_layers: int = 2,
        num_decoder_layers: int = 2,
        d_ff: int = 1024,
        dropout: float = 0.1,
        pad_id: int = 0,
        tie_embeddings: bool = True,
        max_len: int = 1024,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.pad_id = pad_id

        self.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        self.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        if tie_embeddings:
            self.tgt_embed.weight = self.src_embed.weight

        self.pos_enc = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)

        self.enc_layers = nn.ModuleList(
            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_encoder_layers)]
        )
        self.dec_layers = nn.ModuleList(
            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_decoder_layers)]
        )

        self.norm_enc = nn.LayerNorm(d_model)
        self.norm_dec = nn.LayerNorm(d_model)
        self.generator = nn.Linear(d_model, vocab_size)

        self._reset_parameters()

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def make_src_key_padding_mask(self, src: torch.Tensor) -> torch.Tensor:
        # src: (B, Ls)
        mask = (src != self.pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,Ls); 1=keep, 0=mask
        return mask

    def make_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:
        # tgt: (B, Lt)
        B, L = tgt.shape
        pad_mask = (tgt != self.pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,L)
        causal_mask = torch.tril(torch.ones(L, L, device=tgt.device)).bool()  # (L,L)
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # (1,1,L,L)
        return pad_mask & causal_mask  # (B,1,L,L)

    def encode(self, src: torch.Tensor) -> torch.Tensor:
        x = self.src_embed(src) * math.sqrt(self.d_model)
        x = self.pos_enc(x)
        mask = self.make_src_key_padding_mask(src)
        for layer in self.enc_layers:
            x = layer(x, mask)
        return self.norm_enc(x)

    def decode(self, tgt: torch.Tensor, mem: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:
        x = self.tgt_embed(tgt) * math.sqrt(self.d_model)
        x = self.pos_enc(x)
        tgt_mask = self.make_tgt_mask(tgt)
        for layer in self.dec_layers:
            x = layer(x, mem, tgt_mask, src_mask)
        return self.norm_dec(x)

    def forward(self, src: torch.Tensor, tgt_inp: torch.Tensor) -> torch.Tensor:
        # src: (B,Ls); tgt_inp: (B,Lt) starts with <sos>
        mem = self.encode(src)
        src_mask = self.make_src_key_padding_mask(src)
        dec_out = self.decode(tgt_inp, mem, src_mask)
        logits = self.generator(dec_out)  # (B,Lt,V)
        return logits

    @torch.no_grad()
    def greedy_decode(self, src: torch.Tensor, sos_id: int, eos_id: int, max_len: int = 128) -> torch.Tensor:
        device = src.device
        mem = self.encode(src)
        src_mask = self.make_src_key_padding_mask(src)

        B = src.size(0)
        ys = torch.full((B, 1), sos_id, dtype=torch.long, device=device)
        for _ in range(max_len - 1):
            dec_out = self.decode(ys, mem, src_mask)
            logits = self.generator(dec_out[:, -1:, :])  # (B,1,V)
            next_tok = torch.argmax(logits, dim=-1)  # (B,1)
            ys = torch.cat([ys, next_tok], dim=1)
            if (next_tok == eos_id).all():
                break
        return ys  # includes <sos> ... <eos>

import json
from typing import List, Dict, Tuple, Optional
import torch
from torch.utils.data import Dataset, DataLoader
import random
import os


def load_vocab(vocab_dir: str):
    with open(os.path.join(vocab_dir, "char2id.json"), "r", encoding="utf-8") as f:
        char2id = json.load(f)
    with open(os.path.join(vocab_dir, "id2char.json"), "r", encoding="utf-8") as f:
        id2char = json.load(f)
    with open(os.path.join(vocab_dir, "tokenizer_config.json"), "r", encoding="utf-8") as f:
        tok_conf = json.load(f)
    return char2id, id2char, tok_conf


def read_jsonl(path: str) -> List[Dict]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rows.append(json.loads(line))
    return rows


def encode_text(text: str, char2id: Dict[str, int], unk_id: int) -> List[int]:
    return [char2id.get(ch, unk_id) for ch in text]


class PrefixToSuffixDataset(Dataset):
    """
    Creates input-target pairs by splitting each sentence into prefix (encoder input)
    and suffix (decoder target). This is a self-supervised objective suitable for your single-sentence data.
    """
    def __init__(
        self,
        split_path: str,
        char2id: Dict[str, int],
        pad_id: int,
        sos_id: int,
        eos_id: int,
        unk_id: int,
        split_ratio: float = 0.5,
        min_src_len: int = 3,
        min_tgt_len: int = 3,
        seed: int = 42,
    ):
        super().__init__()
        random.seed(seed)
        self.rows = read_jsonl(split_path)  # each row has {"id": ..., "text": "..."} normalized
        self.char2id = char2id
        self.pad_id = pad_id
        self.sos_id = sos_id
        self.eos_id = eos_id
        self.unk_id = unk_id
        self.split_ratio = split_ratio
        self.min_src_len = min_src_len
        self.min_tgt_len = min_tgt_len

        self.examples: List[Tuple[List[int], List[int]]] = []
        for r in self.rows:
            text = r["text"]
            ids = encode_text(text, char2id, unk_id)
            if len(ids) < (min_src_len + min_tgt_len):
                continue
            k = max(min_src_len, min(int(len(ids) * split_ratio), len(ids) - min_tgt_len))
            src = ids[:k]
            tgt = ids[k:]  # decoder needs <sos> + tgt and labels = tgt + <eos>
            if len(src) >= min_src_len and len(tgt) >= min_tgt_len:
                self.examples.append((src, tgt))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx: int):
        src, tgt = self.examples[idx]
        tgt_inp = [self.sos_id] + tgt
        tgt_out = tgt + [self.eos_id]
        return {
            "src": torch.tensor(src, dtype=torch.long),
            "tgt_inp": torch.tensor(tgt_inp, dtype=torch.long),
            "tgt_out": torch.tensor(tgt_out, dtype=torch.long),
        }


def pad_sequence(seqs: List[torch.Tensor], pad_value: int) -> torch.Tensor:
    max_len = max(s.size(0) for s in seqs)
    out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)
    for i, s in enumerate(seqs):
        out[i, : s.size(0)] = s
    return out


def collate_fn(batch: List[Dict], pad_id: int):
    srcs = [b["src"] for b in batch]
    tgt_inps = [b["tgt_inp"] for b in batch]
    tgt_outs = [b["tgt_out"] for b in batch]
    src = pad_sequence(srcs, pad_id)
    tgt_inp = pad_sequence(tgt_inps, pad_id)
    tgt_out = pad_sequence(tgt_outs, pad_id)
    return src, tgt_inp, tgt_out


def make_loaders(
    data_dir: str,
    batch_size: int,
    split_ratio: float,
    seed: int,
    num_workers: int = 0,
):
    vocab_dir = os.path.join(data_dir, "vocab")
    splits_dir = os.path.join(data_dir, "splits")
    char2id, id2char, tok_conf = load_vocab(vocab_dir)
    pad_id = tok_conf["special_token_ids"]["pad_id"]
    sos_id = tok_conf["special_token_ids"]["sos_id"]
    eos_id = tok_conf["special_token_ids"]["eos_id"]
    unk_id = tok_conf["special_token_ids"]["unk_id"]
    vocab_size = tok_conf["vocab_size"]

    train_ds = PrefixToSuffixDataset(
        os.path.join(splits_dir, "train.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
        split_ratio=split_ratio, seed=seed
    )
    val_ds = PrefixToSuffixDataset(
        os.path.join(splits_dir, "val.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
        split_ratio=split_ratio, seed=seed
    )
    test_ds = PrefixToSuffixDataset(
        os.path.join(splits_dir, "test.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
        split_ratio=split_ratio, seed=seed
    )

    collate = lambda b: collate_fn(b, pad_id)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate, num_workers=num_workers)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate, num_workers=num_workers)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate, num_workers=num_workers)

    return {
        "train": train_loader, "val": val_loader, "test": test_loader
    }, {
        "char2id": char2id, "id2char": id2char, "tok_conf": tok_conf, "vocab_size": vocab_size
    }


# ---------------- Metrics ----------------
# BLEU & chrF via sacrebleu; ROUGE-L via simple LCS; Perplexity from loss

def seqs_to_strs(seqs: List[List[int]], id2char: List[str]) -> List[str]:
    # Convert id sequences to strings (stop at eos if present)
    out = []
    for s in seqs:
        chars = [id2char[i] for i in s]
        out.append("".join(chars))
    return out


def lcs_len(a: str, b: str) -> int:
    # Character-level LCS
    la, lb = len(a), len(b)
    dp = [0] * (lb + 1)
    for i in range(1, la + 1):
        prev = 0
        for j in range(1, lb + 1):
            tmp = dp[j]
            if a[i - 1] == b[j - 1]:
                dp[j] = prev + 1
            else:
                dp[j] = max(dp[j], dp[j - 1])
            prev = tmp
    return dp[-1]


def rouge_l(hyps: List[str], refs: List[str]) -> float:
    # F1 over character-level LCS
    scores = []
    for h, r in zip(hyps, refs):
        if len(h) == 0 or len(r) == 0:
            scores.append(0.0)
            continue
        L = lcs_len(h, r)
        prec = L / max(1, len(h))
        rec = L / max(1, len(r))
        if prec + rec == 0:
            scores.append(0.0)
        else:
            scores.append((2 * prec * rec) / (prec + rec))
    return float(sum(scores) / max(1, len(scores)))


def sacrebleu_bleu(hyps: List[str], refs: List[str]) -> float:
    try:
        import sacrebleu
        # treat each character as a token by inserting spaces
        hyps_tok = [" ".join(list(h)) for h in hyps]
        refs_tok = [[" ".join(list(r)) for r in refs]]
        bleu = sacrebleu.corpus_bleu(hyps_tok, refs_tok).score
        return float(bleu)
    except Exception:
        # fallback simple approximate BLEU: not strictly correct
        return 0.0


def sacrebleu_chrf(hyps: List[str], refs: List[str]) -> float:
    try:
        import sacrebleu
        chrf = sacrebleu.corpus_chrf(hyps, [refs]).score
        return float(chrf)
    except Exception:
        return 0.0

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/model_transformer_char.py
# import math
# from typing import Optional, Tuple
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# 
# 
# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
#         super().__init__()
#         self.dropout = nn.Dropout(dropout)
#         pe = torch.zeros(max_len, d_model)  # (L, d_model)
#         position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
#         div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term[: (d_model // 2)])
#         self.register_buffer("pe", pe.unsqueeze(0))  # (1, L, d_model)
# 
#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         # x: (B, L, d_model)
#         x = x + self.pe[:, : x.size(1), :]
#         return self.dropout(x)
# 
# 
# class MultiHeadAttention(nn.Module):
#     def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
#         super().__init__()
#         assert d_model % n_heads == 0
#         self.d_model = d_model
#         self.n_heads = n_heads
#         self.d_head = d_model // n_heads
# 
#         self.q_proj = nn.Linear(d_model, d_model, bias=False)
#         self.k_proj = nn.Linear(d_model, d_model, bias=False)
#         self.v_proj = nn.Linear(d_model, d_model, bias=False)
#         self.o_proj = nn.Linear(d_model, d_model, bias=False)
#         self.attn_drop = nn.Dropout(dropout)
#         self.proj_drop = nn.Dropout(dropout)
# 
#     def forward(
#         self,
#         q: torch.Tensor,  # (B, Lq, d_model)
#         k: torch.Tensor,  # (B, Lk, d_model)
#         v: torch.Tensor,  # (B, Lv, d_model)
#         attn_mask: Optional[torch.Tensor] = None,  # broadcastable to (B, nH, Lq, Lk)
#     ) -> torch.Tensor:
#         B, Lq, _ = q.shape
#         Lk = k.size(1)
# 
#         q = self.q_proj(q).view(B, Lq, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lq, dH)
#         k = self.k_proj(k).view(B, Lk, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lk, dH)
#         v = self.v_proj(v).view(B, Lk, self.n_heads, self.d_head).transpose(1, 2)  # (B, nH, Lk, dH)
# 
#         scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, nH, Lq, Lk)
#         if attn_mask is not None:
#             scores = scores.masked_fill(attn_mask == 0, float("-inf"))
#         attn = torch.softmax(scores, dim=-1)
#         attn = self.attn_drop(attn)
#         out = torch.matmul(attn, v)  # (B, nH, Lq, dH)
#         out = out.transpose(1, 2).contiguous().view(B, Lq, self.d_model)
#         out = self.o_proj(out)
#         return self.proj_drop(out)
# 
# 
# class FeedForward(nn.Module):
#     def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
#         super().__init__()
#         self.fc1 = nn.Linear(d_model, d_ff)
#         self.fc2 = nn.Linear(d_ff, d_model)
#         self.drop = nn.Dropout(dropout)
# 
#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         return self.fc2(self.drop(F.gelu(self.fc1(x))))
# 
# 
# class EncoderLayer(nn.Module):
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):
#         super().__init__()
#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
#         self.ff = FeedForward(d_model, d_ff, dropout)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.drop = nn.Dropout(dropout)
# 
#     def forward(self, x: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:
#         # x: (B, Ls, d_model); src_key_padding_mask: (B, 1, 1, Ls)
#         attn_out = self.self_attn(x, x, x, attn_mask=src_key_padding_mask)
#         x = self.norm1(x + self.drop(attn_out))
#         ff_out = self.ff(x)
#         x = self.norm2(x + self.drop(ff_out))
#         return x
# 
# 
# class DecoderLayer(nn.Module):
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):
#         super().__init__()
#         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
#         self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
#         self.ff = FeedForward(d_model, d_ff, dropout)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.norm3 = nn.LayerNorm(d_model)
#         self.drop = nn.Dropout(dropout)
# 
#     def forward(
#         self,
#         x: torch.Tensor,  # (B, Lt, d_model)
#         mem: torch.Tensor,  # (B, Ls, d_model)
#         tgt_mask: Optional[torch.Tensor],  # (B, 1, Lt, Lt)
#         src_key_padding_mask: Optional[torch.Tensor],  # (B, 1, 1, Ls)
#     ) -> torch.Tensor:
#         # masked self-attention
#         self_attn_out = self.self_attn(x, x, x, attn_mask=tgt_mask)
#         x = self.norm1(x + self.drop(self_attn_out))
#         # cross-attention: query=decoder, key/value=encoder memory
#         cross_attn_out = self.cross_attn(x, mem, mem, attn_mask=src_key_padding_mask)
#         x = self.norm2(x + self.drop(cross_attn_out))
#         ff_out = self.ff(x)
#         x = self.norm3(x + self.drop(ff_out))
#         return x
# 
# 
# class TransformerSeq2Seq(nn.Module):
#     def __init__(
#         self,
#         vocab_size: int,
#         d_model: int = 256,
#         n_heads: int = 2,
#         num_encoder_layers: int = 2,
#         num_decoder_layers: int = 2,
#         d_ff: int = 1024,
#         dropout: float = 0.1,
#         pad_id: int = 0,
#         tie_embeddings: bool = True,
#         max_len: int = 1024,
#     ):
#         super().__init__()
#         self.vocab_size = vocab_size
#         self.d_model = d_model
#         self.pad_id = pad_id
# 
#         self.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
#         self.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
#         if tie_embeddings:
#             self.tgt_embed.weight = self.src_embed.weight
# 
#         self.pos_enc = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)
# 
#         self.enc_layers = nn.ModuleList(
#             [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_encoder_layers)]
#         )
#         self.dec_layers = nn.ModuleList(
#             [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_decoder_layers)]
#         )
# 
#         self.norm_enc = nn.LayerNorm(d_model)
#         self.norm_dec = nn.LayerNorm(d_model)
#         self.generator = nn.Linear(d_model, vocab_size)
# 
#         self._reset_parameters()
# 
#     def _reset_parameters(self):
#         for p in self.parameters():
#             if p.dim() > 1:
#                 nn.init.xavier_uniform_(p)
# 
#     def make_src_key_padding_mask(self, src: torch.Tensor) -> torch.Tensor:
#         # src: (B, Ls)
#         mask = (src != self.pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,Ls); 1=keep, 0=mask
#         return mask
# 
#     def make_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:
#         # tgt: (B, Lt)
#         B, L = tgt.shape
#         pad_mask = (tgt != self.pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,L)
#         causal_mask = torch.tril(torch.ones(L, L, device=tgt.device)).bool()  # (L,L)
#         causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # (1,1,L,L)
#         return pad_mask & causal_mask  # (B,1,L,L)
# 
#     def encode(self, src: torch.Tensor) -> torch.Tensor:
#         x = self.src_embed(src) * math.sqrt(self.d_model)
#         x = self.pos_enc(x)
#         mask = self.make_src_key_padding_mask(src)
#         for layer in self.enc_layers:
#             x = layer(x, mask)
#         return self.norm_enc(x)
# 
#     def decode(self, tgt: torch.Tensor, mem: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:
#         x = self.tgt_embed(tgt) * math.sqrt(self.d_model)
#         x = self.pos_enc(x)
#         tgt_mask = self.make_tgt_mask(tgt)
#         for layer in self.dec_layers:
#             x = layer(x, mem, tgt_mask, src_mask)
#         return self.norm_dec(x)
# 
#     def forward(self, src: torch.Tensor, tgt_inp: torch.Tensor) -> torch.Tensor:
#         # src: (B,Ls); tgt_inp: (B,Lt) starts with <sos>
#         mem = self.encode(src)
#         src_mask = self.make_src_key_padding_mask(src)
#         dec_out = self.decode(tgt_inp, mem, src_mask)
#         logits = self.generator(dec_out)  # (B,Lt,V)
#         return logits
# 
#     @torch.no_grad()
#     def greedy_decode(self, src: torch.Tensor, sos_id: int, eos_id: int, max_len: int = 128) -> torch.Tensor:
#         device = src.device
#         mem = self.encode(src)
#         src_mask = self.make_src_key_padding_mask(src)
# 
#         B = src.size(0)
#         ys = torch.full((B, 1), sos_id, dtype=torch.long, device=device)
#         for _ in range(max_len - 1):
#             dec_out = self.decode(ys, mem, src_mask)
#             logits = self.generator(dec_out[:, -1:, :])  # (B,1,V)
#             next_tok = torch.argmax(logits, dim=-1)  # (B,1)
#             ys = torch.cat([ys, next_tok], dim=1)
#             if (next_tok == eos_id).all():
#                 break
#         return ys  # includes <sos> ... <eos>

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/data_char_loader.py
# import json
# from typing import List, Dict, Tuple, Optional
# import torch
# from torch.utils.data import Dataset, DataLoader
# import random
# import os
# 
# 
# def load_vocab(vocab_dir: str):
#     with open(os.path.join(vocab_dir, "char2id.json"), "r", encoding="utf-8") as f:
#         char2id = json.load(f)
#     with open(os.path.join(vocab_dir, "id2char.json"), "r", encoding="utf-8") as f:
#         id2char = json.load(f)
#     with open(os.path.join(vocab_dir, "tokenizer_config.json"), "r", encoding="utf-8") as f:
#         tok_conf = json.load(f)
#     return char2id, id2char, tok_conf
# 
# 
# def read_jsonl(path: str) -> List[Dict]:
#     rows = []
#     with open(path, "r", encoding="utf-8") as f:
#         for line in f:
#             rows.append(json.loads(line))
#     return rows
# 
# 
# def encode_text(text: str, char2id: Dict[str, int], unk_id: int) -> List[int]:
#     return [char2id.get(ch, unk_id) for ch in text]
# 
# 
# class PrefixToSuffixDataset(Dataset):
#     """
#     Creates input-target pairs by splitting each sentence into prefix (encoder input)
#     and suffix (decoder target). This is a self-supervised objective suitable for your single-sentence data.
#     """
#     def __init__(
#         self,
#         split_path: str,
#         char2id: Dict[str, int],
#         pad_id: int,
#         sos_id: int,
#         eos_id: int,
#         unk_id: int,
#         split_ratio: float = 0.5,
#         min_src_len: int = 3,
#         min_tgt_len: int = 3,
#         seed: int = 42,
#     ):
#         super().__init__()
#         random.seed(seed)
#         self.rows = read_jsonl(split_path)  # each row has {"id": ..., "text": "..."} normalized
#         self.char2id = char2id
#         self.pad_id = pad_id
#         self.sos_id = sos_id
#         self.eos_id = eos_id
#         self.unk_id = unk_id
#         self.split_ratio = split_ratio
#         self.min_src_len = min_src_len
#         self.min_tgt_len = min_tgt_len
# 
#         self.examples: List[Tuple[List[int], List[int]]] = []
#         for r in self.rows:
#             text = r["text"]
#             ids = encode_text(text, char2id, unk_id)
#             if len(ids) < (min_src_len + min_tgt_len):
#                 continue
#             k = max(min_src_len, min(int(len(ids) * split_ratio), len(ids) - min_tgt_len))
#             src = ids[:k]
#             tgt = ids[k:]  # decoder needs <sos> + tgt and labels = tgt + <eos>
#             if len(src) >= min_src_len and len(tgt) >= min_tgt_len:
#                 self.examples.append((src, tgt))
# 
#     def __len__(self):
#         return len(self.examples)
# 
#     def __getitem__(self, idx: int):
#         src, tgt = self.examples[idx]
#         tgt_inp = [self.sos_id] + tgt
#         tgt_out = tgt + [self.eos_id]
#         return {
#             "src": torch.tensor(src, dtype=torch.long),
#             "tgt_inp": torch.tensor(tgt_inp, dtype=torch.long),
#             "tgt_out": torch.tensor(tgt_out, dtype=torch.long),
#         }
# 
# 
# def pad_sequence(seqs: List[torch.Tensor], pad_value: int) -> torch.Tensor:
#     max_len = max(s.size(0) for s in seqs)
#     out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)
#     for i, s in enumerate(seqs):
#         out[i, : s.size(0)] = s
#     return out
# 
# 
# def collate_fn(batch: List[Dict], pad_id: int):
#     srcs = [b["src"] for b in batch]
#     tgt_inps = [b["tgt_inp"] for b in batch]
#     tgt_outs = [b["tgt_out"] for b in batch]
#     src = pad_sequence(srcs, pad_id)
#     tgt_inp = pad_sequence(tgt_inps, pad_id)
#     tgt_out = pad_sequence(tgt_outs, pad_id)
#     return src, tgt_inp, tgt_out
# 
# 
# def make_loaders(
#     data_dir: str,
#     batch_size: int,
#     split_ratio: float,
#     seed: int,
#     num_workers: int = 0,
# ):
#     vocab_dir = os.path.join(data_dir, "vocab")
#     splits_dir = os.path.join(data_dir, "splits")
#     char2id, id2char, tok_conf = load_vocab(vocab_dir)
#     pad_id = tok_conf["special_token_ids"]["pad_id"]
#     sos_id = tok_conf["special_token_ids"]["sos_id"]
#     eos_id = tok_conf["special_token_ids"]["eos_id"]
#     unk_id = tok_conf["special_token_ids"]["unk_id"]
#     vocab_size = tok_conf["vocab_size"]
# 
#     train_ds = PrefixToSuffixDataset(
#         os.path.join(splits_dir, "train.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
#         split_ratio=split_ratio, seed=seed
#     )
#     val_ds = PrefixToSuffixDataset(
#         os.path.join(splits_dir, "val.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
#         split_ratio=split_ratio, seed=seed
#     )
#     test_ds = PrefixToSuffixDataset(
#         os.path.join(splits_dir, "test.jsonl"), char2id, pad_id, sos_id, eos_id, unk_id,
#         split_ratio=split_ratio, seed=seed
#     )
# 
#     collate = lambda b: collate_fn(b, pad_id)
#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate, num_workers=num_workers)
#     val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate, num_workers=num_workers)
#     test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate, num_workers=num_workers)
# 
#     return {
#         "train": train_loader, "val": val_loader, "test": test_loader
#     }, {
#         "char2id": char2id, "id2char": id2char, "tok_conf": tok_conf, "vocab_size": vocab_size
#     }
# 
# 
# # ---------------- Metrics ----------------
# # BLEU & chrF via sacrebleu; ROUGE-L via simple LCS; Perplexity from loss
# 
# def seqs_to_strs(seqs: List[List[int]], id2char: List[str]) -> List[str]:
#     # Convert id sequences to strings (stop at eos if present)
#     out = []
#     for s in seqs:
#         chars = [id2char[i] for i in s]
#         out.append("".join(chars))
#     return out
# 
# 
# def lcs_len(a: str, b: str) -> int:
#     # Character-level LCS
#     la, lb = len(a), len(b)
#     dp = [0] * (lb + 1)
#     for i in range(1, la + 1):
#         prev = 0
#         for j in range(1, lb + 1):
#             tmp = dp[j]
#             if a[i - 1] == b[j - 1]:
#                 dp[j] = prev + 1
#             else:
#                 dp[j] = max(dp[j], dp[j - 1])
#             prev = tmp
#     return dp[-1]
# 
# 
# def rouge_l(hyps: List[str], refs: List[str]) -> float:
#     # F1 over character-level LCS
#     scores = []
#     for h, r in zip(hyps, refs):
#         if len(h) == 0 or len(r) == 0:
#             scores.append(0.0)
#             continue
#         L = lcs_len(h, r)
#         prec = L / max(1, len(h))
#         rec = L / max(1, len(r))
#         if prec + rec == 0:
#             scores.append(0.0)
#         else:
#             scores.append((2 * prec * rec) / (prec + rec))
#     return float(sum(scores) / max(1, len(scores)))
# 
# 
# def sacrebleu_bleu(hyps: List[str], refs: List[str]) -> float:
#     try:
#         import sacrebleu
#         # treat each character as a token by inserting spaces
#         hyps_tok = [" ".join(list(h)) for h in hyps]
#         refs_tok = [[" ".join(list(r)) for r in refs]]
#         bleu = sacrebleu.corpus_bleu(hyps_tok, refs_tok).score
#         return float(bleu)
#     except Exception:
#         # fallback simple approximate BLEU: not strictly correct
#         return 0.0
# 
# 
# def sacrebleu_chrf(hyps: List[str], refs: List[str]) -> float:
#     try:
#         import sacrebleu
#         chrf = sacrebleu.corpus_chrf(hyps, [refs]).score
#         return float(chrf)
#     except Exception:
#         return 0.0

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/train_transformer_char.py
# import argparse
# import os
# import math
# import sys
# from typing import Dict, Any
# import torch
# import torch.nn as nn
# from torch.optim import Adam
# from torch.nn.utils import clip_grad_norm_
# from tqdm import tqdm
# 
# # Ensure we can import sibling modules when running in Colab
# sys.path.append("/content")
# 
# from model_transformer_char import TransformerSeq2Seq
# from data_char_loader import make_loaders, seqs_to_strs, rouge_l, sacrebleu_bleu, sacrebleu_chrf
# 
# 
# def set_seed(seed: int):
#     import random
#     import numpy as np
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
# 
# 
# def compute_loss(logits: torch.Tensor, tgt_out: torch.Tensor, pad_id: int, label_smoothing: float = 0.0):
#     # logits: (B,L,V); tgt_out: (B,L)
#     B, L, V = logits.shape
#     logits = logits.view(B * L, V)
#     tgt_out = tgt_out.view(B * L)
#     if label_smoothing > 0.0:
#         # label smoothing CE
#         with torch.no_grad():
#             true_dist = torch.zeros_like(logits)
#             true_dist.fill_(label_smoothing / (V - 1))
#             true_dist.scatter_(1, tgt_out.unsqueeze(1), 1.0 - label_smoothing)
#             true_dist[tgt_out == pad_id] = 0.0
#         log_probs = torch.log_softmax(logits, dim=-1)
#         loss = -(true_dist * log_probs).sum(dim=-1)
#         loss = loss[tgt_out != pad_id].mean()
#     else:
#         loss = nn.functional.cross_entropy(logits, tgt_out, ignore_index=pad_id)
#     return loss
# 
# 
# @torch.no_grad()
# def evaluate(model, loader, device, ids, max_gen_len=128):
#     model.eval()
#     pad_id = ids["tok_conf"]["special_token_ids"]["pad_id"]
#     sos_id = ids["tok_conf"]["special_token_ids"]["sos_id"]
#     eos_id = ids["tok_conf"]["special_token_ids"]["eos_id"]
#     id2char = ids["id2char"]
# 
#     total_loss = 0.0
#     n_tokens = 0
#     ce_loss = nn.CrossEntropyLoss(ignore_index=pad_id)
# 
#     hyps = []
#     refs = []
# 
#     for src, tgt_inp, tgt_out in loader:
#         src = src.to(device)
#         tgt_inp = tgt_inp.to(device)
#         tgt_out = tgt_out.to(device)
# 
#         logits = model(src, tgt_inp)
#         loss = ce_loss(logits.view(-1, logits.size(-1)), tgt_out.view(-1))
#         tokens = (tgt_out != pad_id).sum().item()
#         total_loss += loss.item() * tokens
#         n_tokens += tokens
# 
#         # Greedy decode from the same src for qualitative + metrics
#         gen = model.greedy_decode(src, sos_id, eos_id, max_len=max_gen_len)  # (B, Lgen)
#         # Remove leading sos and stop at eos for display
#         gen_trim = []
#         for g in gen.tolist():
#             # drop first token (sos), and cut at eos
#             g = g[1:]
#             if eos_id in g:
#                 g = g[: g.index(eos_id)]
#             gen_trim.append(g)
#         tgt_trim = []
#         for t in tgt_out.tolist():
#             # stop at eos if exists (tgt_out already ends with eos or pad)
#             if eos_id in t:
#                 t = t[: t.index(eos_id)]
#             # also remove pad
#             t = [x for x in t if x != pad_id]
#             tgt_trim.append(t)
# 
#         hyps.extend(seqs_to_strs(gen_trim, id2char))
#         refs.extend(seqs_to_strs(tgt_trim, id2char))
# 
#     avg_loss = total_loss / max(1, n_tokens)
#     ppl = math.exp(avg_loss) if avg_loss < 20 else float("inf")
#     bleu = sacrebleu_bleu(hyps, refs)
#     chrf = sacrebleu_chrf(hyps, refs)
#     rouge = rouge_l(hyps, refs)
#     return {
#         "loss": avg_loss,
#         "ppl": ppl,
#         "bleu": bleu,
#         "chrf": chrf,
#         "rougeL": rouge,
#         "samples": list(zip(hyps[:5], refs[:5])),
#     }
# 
# 
# def train(cfg: Dict[str, Any]):
#     set_seed(cfg["seed"])
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     loaders, ids = make_loaders(
#         cfg["data_dir"], cfg["batch_size"], cfg["split_ratio"], cfg["seed"], num_workers=0
#     )
#     pad_id = ids["tok_conf"]["special_token_ids"]["pad_id"]
# 
#     model = TransformerSeq2Seq(
#         vocab_size=ids["vocab_size"],
#         d_model=cfg["d_model"],
#         n_heads=cfg["n_heads"],
#         num_encoder_layers=cfg["enc_layers"],
#         num_decoder_layers=cfg["dec_layers"],
#         d_ff=cfg["d_ff"],
#         dropout=cfg["dropout"],
#         pad_id=pad_id,
#         tie_embeddings=True,
#         max_len=cfg["max_len"],
#     ).to(device)
# 
#     optimizer = Adam(model.parameters(), lr=cfg["lr"], betas=(0.9, 0.98), eps=1e-9)
#     best_bleu = -1.0
#     os.makedirs(cfg["out_dir"], exist_ok=True)
# 
#     global_step = 0
#     for epoch in range(1, cfg["epochs"] + 1):
#         model.train()
#         pbar = tqdm(loaders["train"], desc=f"Epoch {epoch}", dynamic_ncols=True)
#         running_loss = 0.0
#         running_tokens = 0
# 
#         for src, tgt_inp, tgt_out in pbar:
#             src = src.to(device)
#             tgt_inp = tgt_inp.to(device)
#             tgt_out = tgt_out.to(device)
# 
#             logits = model(src, tgt_inp)  # teacher forcing
#             loss = compute_loss(logits, tgt_out, pad_id, label_smoothing=cfg["label_smoothing"])
# 
#             optimizer.zero_grad()
#             loss.backward()
#             clip_grad_norm_(model.parameters(), cfg["clip_norm"])
#             optimizer.step()
# 
#             tokens = (tgt_out != pad_id).sum().item()
#             running_loss += loss.item() * tokens
#             running_tokens += tokens
#             global_step += 1
# 
#             if running_tokens > 0:
#                 pbar.set_postfix(loss=running_loss / running_tokens)
# 
#         # Validation
#         val_metrics = evaluate(model, loaders["val"], device, ids, max_gen_len=cfg["max_len"])
#         print(f"\nVal: loss={val_metrics['loss']:.4f} ppl={val_metrics['ppl']:.2f} "
#               f"BLEU={val_metrics['bleu']:.2f} chrF={val_metrics['chrf']:.2f} ROUGE-L={val_metrics['rougeL']:.3f}")
#         for i, (h, r) in enumerate(val_metrics["samples"], 1):
#             print(f"  Sample {i}\n    pred: {h}\n    ref : {r}")
# 
#         # Save last
#         last_path = os.path.join(cfg["out_dir"], "model_last.pt")
#         torch.save(
#             {
#                 "epoch": epoch,
#                 "model_state": model.state_dict(),
#                 "optimizer_state": optimizer.state_dict(),
#                 "val": val_metrics,
#                 "cfg": cfg,
#             },
#             last_path,
#         )
# 
#         # Save best by BLEU
#         if val_metrics["bleu"] > best_bleu:
#             best_bleu = val_metrics["bleu"]
#             best_path = os.path.join(cfg["out_dir"], "model_best.pt")
#             torch.save(
#                 {
#                     "epoch": epoch,
#                     "model_state": model.state_dict(),
#                     "optimizer_state": optimizer.state_dict(),
#                     "val": val_metrics,
#                     "cfg": cfg,
#                 },
#                 best_path,
#             )
#             print(f"Saved best model to: {best_path} (BLEU={best_bleu:.2f})")
# 
#     # Final evaluation on test with best model
#     best_path = os.path.join(cfg["out_dir"], "model_best.pt")
#     if os.path.exists(best_path):
#         ckpt = torch.load(best_path, map_location=device)
#         model.load_state_dict(ckpt["model_state"])
#         test_metrics = evaluate(model, loaders["test"], device, ids, max_gen_len=cfg["max_len"])
#         print(f"\nTest: loss={test_metrics['loss']:.4f} ppl={test_metrics['ppl']:.2f} "
#               f"BLEU={test_metrics['bleu']:.2f} chrF={test_metrics['chrf']:.2f} ROUGE-L={test_metrics['rougeL']:.3f}")
#         for i, (h, r) in enumerate(test_metrics["samples"], 1):
#             print(f"  Sample {i}\n    pred: {h}\n    ref : {r}")
# 
# 
# def parse_args():
#     ap = argparse.ArgumentParser()
#     ap.add_argument("--data-dir", type=str, required=True, help="Path to data_char directory")
#     ap.add_argument("--out-dir", type=str, required=True, help="Directory to save checkpoints")
#     ap.add_argument("--d-model", type=int, default=256)
#     ap.add_argument("--n-heads", type=int, default=2)
#     ap.add_argument("--enc-layers", type=int, default=2)
#     ap.add_argument("--dec-layers", type=int, default=2)
#     ap.add_argument("--d-ff", type=int, default=1024)
#     ap.add_argument("--dropout", type=float, default=0.1)
#     ap.add_argument("--batch-size", type=int, default=64)
#     ap.add_argument("--lr", type=float, default=3e-4)
#     ap.add_argument("--epochs", type=int, default=10)
#     ap.add_argument("--max-len", type=int, default=256)
#     ap.add_argument("--split-ratio", type=float, default=0.5, help="Prefix ratio (0.5 = half prefix)")
#     ap.add_argument("--seed", type=int, default=42)
#     ap.add_argument("--label-smoothing", type=float, default=0.0)
#     ap.add_argument("--clip-norm", type=float, default=1.0)
#     return ap.parse_args()
# 
# 
# if __name__ == "__main__":
#     args = parse_args()
#     cfg = {
#         "data_dir": args.data_dir,
#         "out_dir": args.out_dir,
#         "d_model": args.d_model,
#         "n_heads": args.n_heads,
#         "enc_layers": args.enc_layers,
#         "dec_layers": args.dec_layers,
#         "d_ff": args.d_ff,
#         "dropout": args.dropout,
#         "batch_size": args.batch_size,
#         "lr": args.lr,
#         "epochs": args.epochs,
#         "max_len": args.max_len,
#         "split_ratio": args.split_ratio,
#         "seed": args.seed,
#         "label_smoothing": args.label_smoothing,
#         "clip_norm": args.clip_norm,
#     }
#     train(cfg)

!pip install -q sacrebleu rouge-score tqdm

!ls -l /content/model_transformer_char.py /content/data_char_loader.py /content/train_transformer_char.py

!python /content/train_transformer_char.py --data-dir "/content/data_char" --out-dir "/content/ckpts_char" --d-model 256 --n-heads 2 --enc-layers 2 --dec-layers 2 --dropout 0.1 --batch-size 64 --lr 3e-4 --epochs 10

